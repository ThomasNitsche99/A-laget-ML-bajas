{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>heading</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>shippingLineId</th>\n",
       "      <th>GT</th>\n",
       "      <th>vesselType</th>\n",
       "      <th>depth</th>\n",
       "      <th>draft</th>\n",
       "      <th>enginePower</th>\n",
       "      <th>freshWater</th>\n",
       "      <th>fuel</th>\n",
       "      <th>homePort</th>\n",
       "      <th>maxHeight</th>\n",
       "      <th>maxSpeed</th>\n",
       "      <th>maxWidth</th>\n",
       "      <th>rampCapacity</th>\n",
       "      <th>yearBuilt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131115</th>\n",
       "      <td>2024-01-12 14:07:47</td>\n",
       "      <td>316</td>\n",
       "      <td>7.50361</td>\n",
       "      <td>77.58340</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131279</th>\n",
       "      <td>2024-01-12 14:31:00</td>\n",
       "      <td>313</td>\n",
       "      <td>7.57302</td>\n",
       "      <td>77.49505</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131514</th>\n",
       "      <td>2024-01-12 14:57:23</td>\n",
       "      <td>312</td>\n",
       "      <td>7.65043</td>\n",
       "      <td>77.39404</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131696</th>\n",
       "      <td>2024-01-12 15:18:48</td>\n",
       "      <td>313</td>\n",
       "      <td>7.71275</td>\n",
       "      <td>77.31394</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131885</th>\n",
       "      <td>2024-01-12 15:39:47</td>\n",
       "      <td>313</td>\n",
       "      <td>7.77191</td>\n",
       "      <td>77.23585</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time  heading  latitude  longitude  \\\n",
       "131115 2024-01-12 14:07:47      316   7.50361   77.58340   \n",
       "131279 2024-01-12 14:31:00      313   7.57302   77.49505   \n",
       "131514 2024-01-12 14:57:23      312   7.65043   77.39404   \n",
       "131696 2024-01-12 15:18:48      313   7.71275   77.31394   \n",
       "131885 2024-01-12 15:39:47      313   7.77191   77.23585   \n",
       "\n",
       "                        vesselId            shippingLineId     GT vesselType  \\\n",
       "131115  61e9f38eb937134a3c4bfd8b  61a8e672f9cba188601e84ab  58684       83.0   \n",
       "131279  61e9f38eb937134a3c4bfd8b  61a8e672f9cba188601e84ab  58684       83.0   \n",
       "131514  61e9f38eb937134a3c4bfd8b  61a8e672f9cba188601e84ab  58684       83.0   \n",
       "131696  61e9f38eb937134a3c4bfd8b  61a8e672f9cba188601e84ab  58684       83.0   \n",
       "131885  61e9f38eb937134a3c4bfd8b  61a8e672f9cba188601e84ab  58684       83.0   \n",
       "\n",
       "        depth  draft  enginePower  freshWater  fuel homePort  maxHeight  \\\n",
       "131115   22.2    NaN          0.0         NaN   NaN     OSLO        5.0   \n",
       "131279   22.2    NaN          0.0         NaN   NaN     OSLO        5.0   \n",
       "131514   22.2    NaN          0.0         NaN   NaN     OSLO        5.0   \n",
       "131696   22.2    NaN          0.0         NaN   NaN     OSLO        5.0   \n",
       "131885   22.2    NaN          0.0         NaN   NaN     OSLO        5.0   \n",
       "\n",
       "        maxSpeed  maxWidth  rampCapacity  yearBuilt  \n",
       "131115      18.6      15.2         150.0       2000  \n",
       "131279      18.6      15.2         150.0       2000  \n",
       "131514      18.6      15.2         150.0       2000  \n",
       "131696      18.6      15.2         150.0       2000  \n",
       "131885      18.6      15.2         150.0       2000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load AIS train dataset\n",
    "train_dataset = pd.read_csv('/Users/minasjovik/Projects/machine-learning-project/A-laget-ML-bajas/data/raw/ais_train.csv', sep='|')\n",
    "\n",
    "# Load vessels.csv dataset\n",
    "vessels_df = pd.read_csv('/Users/minasjovik/Projects/machine-learning-project/A-laget-ML-bajas/data/cleaned/cleaned_vessels.csv', delimiter=',')\n",
    "\n",
    "# Drop columns we don't want to use in the AIS dataset\n",
    "train_dataset = train_dataset.drop(['etaRaw', 'portId', 'cog', 'sog', 'rot', 'navstat'], axis=1)\n",
    "\n",
    "# Transform time to datetime\n",
    "train_dataset['time'] = pd.to_datetime(train_dataset['time'])\n",
    "\n",
    "vessels_df = vessels_df.drop(['DWT', 'NT', 'CEU', 'breadth', 'length'], axis=1)\n",
    "\n",
    "# Merge the train dataset with vessels dataset on 'vesselId'\n",
    "merged_dataset = pd.merge(train_dataset, vessels_df, on='vesselId', how='left')\n",
    "\n",
    "# Sort the merged data by vesselId and time\n",
    "merged_dataset = merged_dataset.sort_values(by=['vesselId', 'time'])\n",
    "\n",
    "# Display the first few rows to verify the merge\n",
    "merged_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(df: pd.DataFrame, N_KEEP_PAST: int) -> pd.DataFrame:\n",
    "    data_rows = []  # List to store data rows\n",
    "    unique_vessels = df['vesselId'].unique()  # Get unique vessel IDs\n",
    "\n",
    "    for vessel in tqdm(unique_vessels):\n",
    "        vessel_data = df[df['vesselId'] == vessel].sort_values(by='time').reset_index(drop=True)\n",
    "        total_rows = len(vessel_data)\n",
    "        \n",
    "        if total_rows <= N_KEEP_PAST:\n",
    "            continue\n",
    "\n",
    "        # Convert time to seconds for performance reasons\n",
    "        vessel_data['time_seconds'] = vessel_data['time'].astype(np.int64) // 10**9\n",
    "\n",
    "        # Loop through the dataset only once to collect features\n",
    "        for index in range(N_KEEP_PAST, total_rows):\n",
    "            current_record = vessel_data.iloc[index]\n",
    "            past_records = vessel_data.iloc[index - N_KEEP_PAST:index]\n",
    "\n",
    "            feature_row = {\n",
    "                'vesselId': vessel,\n",
    "                'target_lat': current_record['latitude'],\n",
    "                'target_lon': current_record['longitude'],\n",
    "                'target_time': current_record['time']\n",
    "            }\n",
    "\n",
    "            # Incorporate features from vessels.csv only once per vessel (assuming static)\n",
    "            if not feature_row.get('vesselType'):  # Add only if not already added\n",
    "                for col in vessels_df.columns:\n",
    "                    if col not in ['vesselId']:  # Avoid duplicating the key column\n",
    "                        feature_row[col] = vessel_data[col].iloc[0]\n",
    "\n",
    "            # Collect past features efficiently\n",
    "            time_diffs = (current_record['time_seconds'] - past_records['time_seconds']).values / 60.0\n",
    "            feature_row.update({f'minutes_from_target_{i}': time_diffs[i] for i in range(N_KEEP_PAST)})\n",
    "            feature_row.update({f'lat_{i}': past_records['latitude'].iloc[i] for i in range(N_KEEP_PAST)})\n",
    "            feature_row.update({f'lon_{i}': past_records['longitude'].iloc[i] for i in range(N_KEEP_PAST)})\n",
    "\n",
    "            data_rows.append(feature_row)\n",
    "\n",
    "        # Drop the temporary time column to save memory\n",
    "        vessel_data.drop(columns=['time_seconds'], inplace=True)\n",
    "    \n",
    "    return pd.DataFrame(data_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/688 [00:00<?, ?it/s]/var/folders/yq/llblptc118lf6wvmlzl8550r0000gn/T/ipykernel_7213/2426035312.py:13: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
      "  vessel_data['time_seconds'] = vessel_data['time'].astype(np.int64) // 10**9\n",
      "100%|██████████| 688/688 [10:03<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_train = create_training_data(merged_dataset, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train.to_csv('/Users/minasjovik/Projects/machine-learning-project/A-laget-ML-bajas/solutions/Mina/processed_train_mina.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_training_data(merged_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure that the 'time' column is of datetime type for correct sorting\n",
    "    if not pd.api.types.is_datetime64_any_dtype(merged_df['time']):\n",
    "        merged_df['time'] = pd.to_datetime(merged_df['time'])\n",
    "    \n",
    "    # Sort the DataFrame by 'vesselId' and 'time' in ascending order\n",
    "    sorted_df = merged_df.sort_values(by=['vesselId', 'time'], ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train = sort_training_data(merged_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>time</th>\n",
       "      <th>scaling_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>61e9f3aeb937134a3c4bfe3d</td>\n",
       "      <td>2024-05-08 00:03:16</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61e9f473b937134a3c4c02df</td>\n",
       "      <td>2024-05-08 00:06:17</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>61e9f469b937134a3c4c029b</td>\n",
       "      <td>2024-05-08 00:10:02</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>61e9f45bb937134a3c4c0221</td>\n",
       "      <td>2024-05-08 00:10:34</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8d</td>\n",
       "      <td>2024-05-08 00:12:27</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                  vesselId                time  scaling_factor\n",
       "0   0  61e9f3aeb937134a3c4bfe3d 2024-05-08 00:03:16             0.3\n",
       "1   1  61e9f473b937134a3c4c02df 2024-05-08 00:06:17             0.3\n",
       "2   2  61e9f469b937134a3c4c029b 2024-05-08 00:10:02             0.3\n",
       "3   3  61e9f45bb937134a3c4c0221 2024-05-08 00:10:34             0.3\n",
       "4   4  61e9f38eb937134a3c4bfd8d 2024-05-08 00:12:27             0.3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv('/Users/minasjovik/Projects/machine-learning-project/A-laget-ML-bajas/data/raw/ais_test.csv')\n",
    "\n",
    "test_dataset['time'] = pd.to_datetime(test_dataset['time'])\n",
    "\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_test_data(merged_df: pd.DataFrame, test_df: pd.DataFrame, N_KEEP_PAST: int) -> pd.DataFrame:\n",
    "    data_rows = []  # List to collect all the data rows\n",
    "    unique_vessels = test_df['vesselId'].unique()  # Get unique vessel IDs\n",
    "\n",
    "    # Iterate over each unique vessel\n",
    "    for vessel in tqdm(unique_vessels):\n",
    "        # Filter out the train data for this vessel that is less than the max time in test data\n",
    "        vessel_train_data = merged_df[merged_df['vesselId'] == vessel]\n",
    "        # Sort the train data by time, if not already sorted\n",
    "        vessel_train_data = vessel_train_data.sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "        # Get test data for the current vessel\n",
    "        test_vessel_data = test_df[test_df['vesselId'] == vessel]\n",
    "\n",
    "        for _, test_row in test_vessel_data.iterrows():\n",
    "            target_time = test_row['time']\n",
    "            ID = test_row['ID']\n",
    "            # Get the last N_KEEP_PAST records from the train data before the target time\n",
    "            past_data = vessel_train_data[vessel_train_data['time'] < target_time].tail(N_KEEP_PAST)\n",
    "\n",
    "            # Check if we have enough past data\n",
    "            if len(past_data) < N_KEEP_PAST:\n",
    "                continue  # Not enough past data; skip this test row\n",
    "\n",
    "            # Prepare a dictionary to hold the features\n",
    "            feature_row = {\n",
    "                'vesselId': vessel,\n",
    "                'target_time': target_time,  # Only include target time\n",
    "                'ID': ID\n",
    "            }\n",
    "\n",
    "            # Incorporate vessel-specific features from vessels.csv for the current vessel\n",
    "            vessel_info = vessel_train_data.iloc[0]  # Assume all records for this vessel share the same static features\n",
    "            for col in vessels_df.columns:\n",
    "                if col not in ['vesselId']:  # Avoid duplicating the key column\n",
    "                    feature_row[col] = vessel_info[col]\n",
    "\n",
    "            # Loop through past records to collect features\n",
    "            for j in range(N_KEEP_PAST):\n",
    "                past_record = past_data.iloc[j]\n",
    "                time_diff = (target_time - past_record['time']).total_seconds() / 60.0  # Difference in minutes\n",
    "\n",
    "                # Add features for minutes from target and heading\n",
    "                feature_row[f'minutes_from_target_{j}'] = time_diff\n",
    "                feature_row[f'heading_{j}'] = past_record['heading']\n",
    "                feature_row[f'lat_{j}'] = past_record['latitude']\n",
    "                feature_row[f'lon_{j}'] = past_record['longitude']\n",
    "\n",
    "            # Append the row to the list\n",
    "            data_rows.append(feature_row)\n",
    "\n",
    "    # Convert the list of feature rows to a DataFrame\n",
    "    return pd.DataFrame(data_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [01:11<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_test = create_test_data(sorted_train, test_dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test.to_csv('/Users/minasjovik/Projects/machine-learning-project/A-laget-ML-bajas/solutions/Mina/processed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load datasets into pandas DataFrames\n",
    "train_df = pd.read_csv('/Users/minasjovik/Projects/machine-learning-project/A-laget-ML-bajas/solutions/Mina/processed_train_mina.csv')\n",
    "test_df = pd.read_csv('/Users/minasjovik/Projects/machine-learning-project/A-laget-ML-bajas/solutions/Mina/processed_test.csv')\n",
    "\n",
    "# Encode the vesselId using LabelEncoder\n",
    "le_vessel = LabelEncoder()\n",
    "train_df['vesselId_encoded'] = le_vessel.fit_transform(train_df['vesselId'])\n",
    "test_df['vesselId_encoded'] = le_vessel.transform(test_df['vesselId'])\n",
    "\n",
    "# Drop the original vesselId column now that we have the encoded version\n",
    "train_df = train_df.drop(columns=['vesselId'])\n",
    "test_df = test_df.drop(columns=['vesselId'])\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_cols = train_df.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns in training set:\", non_numeric_cols)\n",
    "\n",
    "# Combine train and test data for consistent encoding\n",
    "combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "\n",
    "for col in non_numeric_cols:\n",
    "    nunique = combined_df[col].nunique()\n",
    "    if nunique < 100:\n",
    "        # One-hot encode columns with few unique values\n",
    "        combined_df = pd.get_dummies(combined_df, columns=[col], prefix=col)\n",
    "    else:\n",
    "        # Apply Label Encoding for high-cardinality columns\n",
    "        le = LabelEncoder()\n",
    "        combined_df[col] = le.fit_transform(combined_df[col].astype(str))\n",
    "\n",
    "# Split the combined data back into train and test sets\n",
    "train_df = combined_df.xs('train')\n",
    "test_df = combined_df.xs('test')\n",
    "\n",
    "# Ensure that the target columns are not included in the test set\n",
    "target_columns = ['target_lat', 'target_lon', 'target_time']\n",
    "X = train_df.drop(columns=target_columns)\n",
    "y = train_df[['target_lat', 'target_lon']]\n",
    "X_test = test_df[X.columns]  # Ensure X_test has the same columns as X\n",
    "\n",
    "# Initialize XGBoost model wrapped in a MultiOutputRegressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42)\n",
    "multi_output_model = MultiOutputRegressor(xgb_model)\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize arrays for predictions\n",
    "oof_preds = np.zeros((X.shape[0], y.shape[1]))\n",
    "test_preds = np.zeros((X_test.shape[0], y.shape[1], n_folds))\n",
    "\n",
    "# Perform training and predictions using K-Fold\n",
    "for fold_index, (train_indices, valid_indices) in enumerate(kf.split(X, y)):\n",
    "    X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "    X_val, y_val = X.iloc[valid_indices], y.iloc[valid_indices]\n",
    "    \n",
    "    print(f\"Training fold {fold_index + 1}/{n_folds} using XGBoost...\")\n",
    "    multi_output_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    val_preds = multi_output_model.predict(X_val)\n",
    "    test_fold_preds = multi_output_model.predict(X_test)\n",
    "\n",
    "    # Save the predictions\n",
    "    oof_preds[valid_indices] = val_preds\n",
    "    test_preds[:, :, fold_index] = test_fold_preds\n",
    "\n",
    "# Define the function to compute mean squared error for weight optimization\n",
    "def compute_mse(weights):\n",
    "    normalized_weights = np.array(weights) / np.sum(weights)  # Normalize weights\n",
    "    combined_preds = normalized_weights[0] * oof_preds  # Since there's only one model\n",
    "    mse = mean_squared_error(y, combined_preds)\n",
    "    return mse\n",
    "\n",
    "# Initial weight for optimization\n",
    "initial_weight = [1.0]  # Single model\n",
    "bounds = [(0, 1)]  # Weights should be between 0 and 1\n",
    "\n",
    "# Optimize the weights to minimize the MSE\n",
    "optimal_mse = float('inf')\n",
    "optimal_weights = None\n",
    "\n",
    "print(\"Starting weight optimization...\")\n",
    "try:\n",
    "    optimization_result = minimize(compute_mse, initial_weight, method='Nelder-Mead', bounds=bounds)\n",
    "    optimal_mse = optimization_result.fun\n",
    "    optimal_weights = optimization_result.x / np.sum(optimization_result.x)  # Normalize weights\n",
    "except Exception as e:\n",
    "    print(f\"Error during optimization: {e}\")\n",
    "\n",
    "# Average the test predictions across folds\n",
    "mean_test_predictions = np.mean(test_preds, axis=2)\n",
    "\n",
    "# Finalize predictions using optimized weights\n",
    "final_predictions = optimal_weights[0] * mean_test_predictions  # Since there's only one model\n",
    "\n",
    "# Create a DataFrame for output\n",
    "results_df = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'longitude_predicted': final_predictions[:, 1],\n",
    "    'latitude_predicted': final_predictions[:, 0]\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "results_df.to_csv('/Users/minasjovik/Projects/machine-learning-project/A-laget-ML-bajas/solutions/Mina/predictions6.csv', index=False)\n",
    "\n",
    "# Print optimized results\n",
    "print(f\"Optimized Mean Squared Error: {optimal_mse}\")\n",
    "print(f\"Final Prediction Weight: {optimal_weights[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maskinlaering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
