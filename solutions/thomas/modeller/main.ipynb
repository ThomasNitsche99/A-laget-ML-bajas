{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in train data and sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>cog</th>\n",
       "      <th>sog</th>\n",
       "      <th>rot</th>\n",
       "      <th>heading</th>\n",
       "      <th>navstat</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>vesselId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131115</th>\n",
       "      <td>2024-01-12 14:07:47</td>\n",
       "      <td>308.1</td>\n",
       "      <td>17.1</td>\n",
       "      <td>-6</td>\n",
       "      <td>316</td>\n",
       "      <td>0</td>\n",
       "      <td>7.50361</td>\n",
       "      <td>77.58340</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131279</th>\n",
       "      <td>2024-01-12 14:31:00</td>\n",
       "      <td>307.6</td>\n",
       "      <td>17.3</td>\n",
       "      <td>5</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "      <td>7.57302</td>\n",
       "      <td>77.49505</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131514</th>\n",
       "      <td>2024-01-12 14:57:23</td>\n",
       "      <td>306.8</td>\n",
       "      <td>16.9</td>\n",
       "      <td>5</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>7.65043</td>\n",
       "      <td>77.39404</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131696</th>\n",
       "      <td>2024-01-12 15:18:48</td>\n",
       "      <td>307.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>6</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "      <td>7.71275</td>\n",
       "      <td>77.31394</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131885</th>\n",
       "      <td>2024-01-12 15:39:47</td>\n",
       "      <td>307.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>7</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "      <td>7.77191</td>\n",
       "      <td>77.23585</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132038</th>\n",
       "      <td>2024-01-12 15:54:48</td>\n",
       "      <td>307.6</td>\n",
       "      <td>16.1</td>\n",
       "      <td>5</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "      <td>7.81285</td>\n",
       "      <td>77.18147</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132237</th>\n",
       "      <td>2024-01-12 16:14:59</td>\n",
       "      <td>309.5</td>\n",
       "      <td>16.1</td>\n",
       "      <td>-6</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "      <td>7.86929</td>\n",
       "      <td>77.11032</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132394</th>\n",
       "      <td>2024-01-12 16:35:24</td>\n",
       "      <td>308.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>7.92585</td>\n",
       "      <td>77.03811</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132538</th>\n",
       "      <td>2024-01-12 16:55:24</td>\n",
       "      <td>310.4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>7.98258</td>\n",
       "      <td>76.96880</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132673</th>\n",
       "      <td>2024-01-12 17:14:36</td>\n",
       "      <td>307.5</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>8.03598</td>\n",
       "      <td>76.90095</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time    cog   sog  rot  heading  navstat  latitude  \\\n",
       "131115 2024-01-12 14:07:47  308.1  17.1   -6      316        0   7.50361   \n",
       "131279 2024-01-12 14:31:00  307.6  17.3    5      313        0   7.57302   \n",
       "131514 2024-01-12 14:57:23  306.8  16.9    5      312        0   7.65043   \n",
       "131696 2024-01-12 15:18:48  307.9  16.9    6      313        0   7.71275   \n",
       "131885 2024-01-12 15:39:47  307.0  16.3    7      313        0   7.77191   \n",
       "132038 2024-01-12 15:54:48  307.6  16.1    5      313        0   7.81285   \n",
       "132237 2024-01-12 16:14:59  309.5  16.1   -6      313        0   7.86929   \n",
       "132394 2024-01-12 16:35:24  308.7  16.0    2      311        0   7.92585   \n",
       "132538 2024-01-12 16:55:24  310.4  16.0   -1      311        0   7.98258   \n",
       "132673 2024-01-12 17:14:36  307.5  16.1    6      307        0   8.03598   \n",
       "\n",
       "        longitude                  vesselId  \n",
       "131115   77.58340  61e9f38eb937134a3c4bfd8b  \n",
       "131279   77.49505  61e9f38eb937134a3c4bfd8b  \n",
       "131514   77.39404  61e9f38eb937134a3c4bfd8b  \n",
       "131696   77.31394  61e9f38eb937134a3c4bfd8b  \n",
       "131885   77.23585  61e9f38eb937134a3c4bfd8b  \n",
       "132038   77.18147  61e9f38eb937134a3c4bfd8b  \n",
       "132237   77.11032  61e9f38eb937134a3c4bfd8b  \n",
       "132394   77.03811  61e9f38eb937134a3c4bfd8b  \n",
       "132538   76.96880  61e9f38eb937134a3c4bfd8b  \n",
       "132673   76.90095  61e9f38eb937134a3c4bfd8b  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/raw/ais_train.csv', sep='|') #Reading train data\n",
    "\n",
    "#train =train.drop(['cog','sog','rot','heading','navstat','portId','etaRaw'], axis=1) # Dropping columns\n",
    "\n",
    "train_data = train_data.drop(['portId','etaRaw'], axis=1) #Dropping portID and etaRaw\n",
    "\n",
    "train_data['time'] = pd.to_datetime(train_data['time']) #Convert time to datetime object\n",
    "\n",
    "train_data = train_data.sort_values(by=['vesselId','time']) #Sort by vesselID and time\n",
    "\n",
    "train_data.head(10) #Printing first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional): Include other datasets in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_data = pd.read_csv('../data/formatted/vessels_converted.csv', sep=',') #Importing vessel data\n",
    "ports_data = None\n",
    "schedules_data = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional vessel): Include vessel data in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vessel_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Merge with training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mmerge(vessel_data, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvesselId\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Handle missing values in vessel data (if any)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# For demonstration, fill missing numerical values with median and categorical with mode\u001b[39;00m\n\u001b[1;32m      6\u001b[0m vessel_numerical_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCEU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDWT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreadth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdraft\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menginePower\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxHeight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxSpeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxWidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrampCapacity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myearBuilt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vessel_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Merge with training data\n",
    "train_data = train_data.merge(vessel_data, on='vesselId', how='left')\n",
    "\n",
    "# Handle missing values in vessel data (if any)\n",
    "# For demonstration, fill missing numerical values with median and categorical with mode\n",
    "vessel_numerical_cols = ['CEU', 'DWT', 'GT', 'NT', 'breadth', 'depth', 'draft', 'enginePower', 'maxHeight', 'maxSpeed', 'maxWidth', 'rampCapacity', 'yearBuilt']\n",
    "vessel_categorical_cols = ['vesselType', 'homePort', 'fuel', 'freshWater']\n",
    "\n",
    "for col in vessel_numerical_cols:\n",
    "    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "\n",
    "for col in vessel_categorical_cols:\n",
    "    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "\n",
    "# Encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Combine train and test vesselType categories to avoid unseen categories in test set\n",
    "vessel_type_le = LabelEncoder()\n",
    "train_data['vesselType_encoded'] = vessel_type_le.fit_transform(train_data['vesselType'])\n",
    "\n",
    "home_port_le = LabelEncoder()\n",
    "train_data['homePort_encoded'] = home_port_le.fit_transform(train_data['homePort'])\n",
    "\n",
    "# Calculate vessel age at the time of each observation\n",
    "train_data['vessel_age'] = train_data['time'].dt.year - train_data['yearBuilt']\n",
    "\n",
    "# Drop original categorical columns if not needed\n",
    "#train = train.drop(['vesselType', 'homePort', 'fuel', 'freshWater', 'yearBuilt'], axis=1)\n",
    "\n",
    "# Reorder columns if desired\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[\"vesselId\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training dataset with past observations\n",
    "\n",
    "`train_df`: The preprocessed training dataframe\n",
    "\n",
    "`N_KEEP_PAST`: The number og past (historical) observations to retain as features\n",
    "\n",
    "The function creates a new dataset containing historical data for each vesselID in the ais_train dataset in a increasing manner. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(train_df: pd.DataFrame, N_KEEP_PAST: int) -> pd.DataFrame:\n",
    "    data_rows = []  # List to collect all the data rows\n",
    "    vessel_list = train_df['vesselId'].unique() # List of all unique vesselIDs\n",
    "    \n",
    "    for vessel in tqdm(vessel_list):\n",
    "        \n",
    "        vessel_data = train_df[train_df['vesselId'] == vessel].sort_values(by='time').reset_index(drop=True) #Sortin the data for each vessel chronologically by time\n",
    "        \n",
    "        num_rows = vessel_data.shape[0] #number of rows for the current vesselID\n",
    "        \n",
    "        if num_rows <= N_KEEP_PAST: # If a vessel have fewer records than N_KEEP_PAST, then skip\n",
    "            continue  # Skip vessels with insufficient data\n",
    "        \n",
    "        for i in range(N_KEEP_PAST, num_rows): # For i in range (5 -> number of rows for that vessel) ---> skipping to first record which has variable histroical records behind\n",
    "            \n",
    "            # Collect past N_KEEP_PAST locations and timestamps\n",
    "            past_data = vessel_data.loc[i - N_KEEP_PAST:i - 1].reset_index(drop=True) #Extracting past N_KEEP_PAST records\n",
    "            \n",
    "            current_data = vessel_data.loc[i] #The current record\n",
    "            \n",
    "            # Prepare a dictionary to hold the features and target\n",
    "            data_row = {}\n",
    "            target_time = current_data['time'] #The current time of the current record\n",
    "            \n",
    "            for j in range(N_KEEP_PAST): #For j in range 0 -> 5\n",
    "                past_time = past_data.loc[j, 'time']\n",
    "                \n",
    "                # Calculate the difference in minutes between past time and target time\n",
    "                time_diff = (target_time - past_time).total_seconds() / 60.0  # Difference in minutes\n",
    "                data_row[f'minutes_from_target_{j}'] = time_diff\n",
    "                data_row[f'lat_{j}'] = past_data.loc[j, 'latitude']\n",
    "                data_row[f'lon_{j}'] = past_data.loc[j, 'longitude']\n",
    "                #######\n",
    "                data_row[f'cog_{j}'] = past_data.loc[j, 'cog']\n",
    "                data_row[f'sog_{j}'] = past_data.loc[j, 'sog']\n",
    "                data_row[f'rot_{j}'] = past_data.loc[j, 'rot']\n",
    "                data_row[f'heading_{j}'] = past_data.loc[j, 'heading']\n",
    "                data_row[f'navstat_{j}'] = past_data.loc[j, 'navstat']\n",
    "                ######\n",
    "                \n",
    "            #Vessel attributes from current record is included (Will trigger errror if not vessel data included in train dataset)\n",
    "            \n",
    "            # for col in ['CEU', 'DWT', 'GT', 'NT', 'breadth', 'depth', 'draft',\n",
    "            #             'enginePower', 'maxHeight', 'maxSpeed', 'maxWidth', 'rampCapacity',\n",
    "            #             'vesselType_encoded', 'homePort_encoded', 'vessel_age']:\n",
    "            #     data_row[col] = current_data[col]\n",
    "                \n",
    "                \n",
    "            # Add current location as the target_\n",
    "            data_row['target_lat'] = current_data['latitude'] #Current latitude\n",
    "            \n",
    "            data_row['target_lon'] = current_data['longitude'] #Current longitude\n",
    "            \n",
    "            data_row['vesselId'] = vessel  # Include vesselId if needed\n",
    "            \n",
    "            data_row['target_time'] = target_time # Remove eventually\n",
    "            \n",
    "            # Append the row to the list\n",
    "            data_rows.append(data_row)\n",
    "            \n",
    "    # Create final DataFrame from the list of data rows\n",
    "    final_df = pd.DataFrame(data_rows)\n",
    "    return final_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional): Sampling dataset to see output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying train\n",
    "train_data = train_data.iloc[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defying N_KEEP_PAST and creating train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 688/688 [08:53<00:00,  1.29it/s]  \n"
     ]
    }
   ],
   "source": [
    "N_KEEP_PAST = 7 #Declare N_KEEP_PAST\n",
    "\n",
    "final_train = create_training_data(train_data, N_KEEP_PAST) #Creating train dataset\n",
    "final_train.to_csv('final_train_data.csv', index=False) #Writing to csv file\n",
    "\n",
    "# print(final_train.shape) #Print dimensions of dataframe\n",
    "# display(final_train.tail()) #Printing tail of dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/raw/ais_test.csv') #Reading test dataset\n",
    "\n",
    "test_data['time'] = pd.to_datetime(test_data['time']) #Converting time column to datetime object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional): Include vessel data in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.merge(vessel_data, on='vesselId', how='left') #Merging with vessels data\n",
    "\n",
    "# Encoding categorical variables in test data (such as in train) \n",
    "#Needs to be excluded when dropping vessel\n",
    "test_data['vesselType_encoded'] = vessel_type_le.transform(test_data['vesselType'])\n",
    "test_data['homePort_encoded'] = home_port_le.transform(test_data['homePort'])\n",
    "\n",
    "\n",
    "for col in vessel_numerical_cols:\n",
    "    test_data[col] = test_data[col].fillna(train_data[col].median())  # Use median from train\n",
    "\n",
    "for col in vessel_categorical_cols:\n",
    "    test_data[col] = test_data[col].fillna(train_data[col].mode()[0]) \n",
    "    \n",
    "if 'yearBuilt' in test_data.columns:\n",
    "    test_data['vessel_age'] = test_data['time'].dt.year - test_data['yearBuilt']\n",
    "else:\n",
    "    print(\"'yearBuilt' is not present in test data after merging.\")\n",
    "    # Decide how to handle this case\n",
    "    # For example, you can set a default vessel age\n",
    "    test_data['vessel_age'] = test_data['time'].dt.year - train_data['yearBuilt'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_features(train_df: pd.DataFrame, test_df: pd.DataFrame, N_KEEP_PAST: int) -> pd.DataFrame:\n",
    "    data_rows = []  # List to collect all the data rows\n",
    "    vessel_list = test_df['vesselId'].unique() #List of all unique vesselIDs\n",
    "    \n",
    "    for vessel in tqdm(vessel_list):\n",
    "        \n",
    "        test_vessel_data = test_df[test_df['vesselId'] == vessel] #Get test data for the current vessel\n",
    "        \n",
    "        for idx, test_row in test_vessel_data.iterrows():\n",
    "            target_time = test_row['time']\n",
    "            ID = test_row['ID']\n",
    "            scaling_factor = test_row['scaling_factor']  # If needed later\n",
    "            \n",
    "            \n",
    "            # Get past data from train_data for this vessel before the target time\n",
    "            vessel_train_data = train_df[(train_df['vesselId'] == vessel) & (train_df['time'] < target_time)] #Retrieving data before target time\n",
    "            vessel_train_data = vessel_train_data.sort_values(by='time').reset_index(drop=True) #Sorted by time\n",
    "            num_past_points = vessel_train_data.shape[0] #Number of past records before target time\n",
    "            \n",
    "            if num_past_points < N_KEEP_PAST:\n",
    "                # Not enough past data; decide how to handle (skip or pad with NaNs)\n",
    "                continue  # Or handle as per your requirement\n",
    "            \n",
    "            # Get the last N_KEEP_PAST records\n",
    "            past_data = vessel_train_data.iloc[-N_KEEP_PAST:].reset_index(drop=True)\n",
    "            \n",
    "            # Prepare a dictionary to hold the features\n",
    "            data_row = {}\n",
    "            for j in range(N_KEEP_PAST):\n",
    "                past_time = past_data.loc[j, 'time']\n",
    "                # Calculate the difference in minutes between past time and target time\n",
    "                time_diff = (target_time - past_time).total_seconds() / 60.0  # Difference in minutes\n",
    "                data_row[f'minutes_from_target_{j}'] = time_diff\n",
    "                data_row[f'lat_{j}'] = past_data.loc[j, 'latitude']\n",
    "                data_row[f'lon_{j}'] = past_data.loc[j, 'longitude']\n",
    "                ###\n",
    "                data_row[f'cog_{j}'] = past_data.loc[j, 'cog']\n",
    "                data_row[f'sog_{j}'] = past_data.loc[j, 'sog']\n",
    "                data_row[f'rot_{j}'] = past_data.loc[j, 'rot']\n",
    "                data_row[f'heading_{j}'] = past_data.loc[j, 'heading']\n",
    "                data_row[f'navstat_{j}'] = past_data.loc[j, 'navstat']\n",
    "                ###\n",
    "            # for col in ['CEU', 'DWT', 'GT', 'NT', 'breadth', 'depth', 'draft',\n",
    "            #             'enginePower', 'maxHeight', 'maxSpeed', 'maxWidth', 'rampCapacity',\n",
    "            #             'vesselType_encoded', 'homePort_encoded', 'vessel_age']:\n",
    "            #     data_row[col] = test_row[col]\n",
    "            # Include vesselId and ID for result matching\n",
    "            data_row['vesselId'] = vessel\n",
    "            data_row['ID'] = ID\n",
    "            # Append the row to the list\n",
    "            data_rows.append(data_row)\n",
    "    \n",
    "    # Create test features DataFrame from the list of data rows\n",
    "    test_features = pd.DataFrame(data_rows)\n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [36:36<00:00, 10.22s/it]\n"
     ]
    }
   ],
   "source": [
    "test_final = create_test_features(train_data, test_data, N_KEEP_PAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minutes_from_target_0</th>\n",
       "      <th>lat_0</th>\n",
       "      <th>lon_0</th>\n",
       "      <th>cog_0</th>\n",
       "      <th>sog_0</th>\n",
       "      <th>rot_0</th>\n",
       "      <th>heading_0</th>\n",
       "      <th>navstat_0</th>\n",
       "      <th>minutes_from_target_1</th>\n",
       "      <th>lat_1</th>\n",
       "      <th>...</th>\n",
       "      <th>minutes_from_target_6</th>\n",
       "      <th>lat_6</th>\n",
       "      <th>lon_6</th>\n",
       "      <th>cog_6</th>\n",
       "      <th>sog_6</th>\n",
       "      <th>rot_6</th>\n",
       "      <th>heading_6</th>\n",
       "      <th>navstat_6</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>846.083333</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>-81.49791</td>\n",
       "      <td>316.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>810.116667</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>...</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>31.14647</td>\n",
       "      <td>-81.49789</td>\n",
       "      <td>179.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>61e9f3aeb937134a3c4bfe3d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>879.050000</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>-81.49791</td>\n",
       "      <td>316.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>843.083333</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>...</td>\n",
       "      <td>47.966667</td>\n",
       "      <td>31.14647</td>\n",
       "      <td>-81.49789</td>\n",
       "      <td>179.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>61e9f3aeb937134a3c4bfe3d</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894.016667</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>-81.49791</td>\n",
       "      <td>316.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>858.050000</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>...</td>\n",
       "      <td>62.933333</td>\n",
       "      <td>31.14647</td>\n",
       "      <td>-81.49789</td>\n",
       "      <td>179.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>61e9f3aeb937134a3c4bfe3d</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>915.050000</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>-81.49791</td>\n",
       "      <td>316.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>879.083333</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>...</td>\n",
       "      <td>83.966667</td>\n",
       "      <td>31.14647</td>\n",
       "      <td>-81.49789</td>\n",
       "      <td>179.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>61e9f3aeb937134a3c4bfe3d</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>933.050000</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>-81.49791</td>\n",
       "      <td>316.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>897.083333</td>\n",
       "      <td>31.14645</td>\n",
       "      <td>...</td>\n",
       "      <td>101.966667</td>\n",
       "      <td>31.14647</td>\n",
       "      <td>-81.49789</td>\n",
       "      <td>179.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "      <td>61e9f3aeb937134a3c4bfe3d</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   minutes_from_target_0     lat_0     lon_0  cog_0  sog_0  rot_0  heading_0  \\\n",
       "0             846.083333  31.14645 -81.49791  316.3    0.0     -4        344   \n",
       "1             879.050000  31.14645 -81.49791  316.3    0.0     -4        344   \n",
       "2             894.016667  31.14645 -81.49791  316.3    0.0     -4        344   \n",
       "3             915.050000  31.14645 -81.49791  316.3    0.0     -4        344   \n",
       "4             933.050000  31.14645 -81.49791  316.3    0.0     -4        344   \n",
       "\n",
       "   navstat_0  minutes_from_target_1     lat_1  ...  minutes_from_target_6  \\\n",
       "0          5             810.116667  31.14645  ...              15.000000   \n",
       "1          5             843.083333  31.14645  ...              47.966667   \n",
       "2          5             858.050000  31.14645  ...              62.933333   \n",
       "3          5             879.083333  31.14645  ...              83.966667   \n",
       "4          5             897.083333  31.14645  ...             101.966667   \n",
       "\n",
       "      lat_6     lon_6  cog_6  sog_6  rot_6  heading_6  navstat_6  \\\n",
       "0  31.14647 -81.49789  179.6    0.0      0        344          5   \n",
       "1  31.14647 -81.49789  179.6    0.0      0        344          5   \n",
       "2  31.14647 -81.49789  179.6    0.0      0        344          5   \n",
       "3  31.14647 -81.49789  179.6    0.0      0        344          5   \n",
       "4  31.14647 -81.49789  179.6    0.0      0        344          5   \n",
       "\n",
       "                   vesselId   ID  \n",
       "0  61e9f3aeb937134a3c4bfe3d    0  \n",
       "1  61e9f3aeb937134a3c4bfe3d  143  \n",
       "2  61e9f3aeb937134a3c4bfe3d  282  \n",
       "3  61e9f3aeb937134a3c4bfe3d  426  \n",
       "4  61e9f3aeb937134a3c4bfe3d  551  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train.to_csv('final_train.csv', index=False)\n",
    "test_final.to_csv('test_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting with model: RandomForest\n",
      "Training and predicting with model: XGBoost\n",
      "Training and predicting with model: ElasticNet\n",
      "Training and predicting with model: LightGBM\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Load the data\n",
    "test_final = pd.read_csv('test_final.csv')\n",
    "final_train = pd.read_csv('final_train.csv')\n",
    "\n",
    "# Define features and targets\n",
    "features = final_train.drop(columns=['target_lat', 'target_lon', 'vesselId', 'target_time'])\n",
    "targets = final_train[['target_lat', 'target_lon']]\n",
    "\n",
    "feature_columns = features.columns\n",
    "X_test = test_final[feature_columns]\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'RandomForest': MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=4, n_jobs=8)),\n",
    "    'XGBoost': MultiOutputRegressor(xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42)),\n",
    "    'ElasticNet': MultiOutputRegressor(ElasticNet(random_state=42)),\n",
    "    'LightGBM': MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))\n",
    "}\n",
    "\n",
    "# Prepare arrays to hold OOF predictions and test predictions\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = {model_name: np.zeros((features.shape[0], targets.shape[1])) for model_name in models}\n",
    "test_preds = {model_name: np.zeros((X_test.shape[0], targets.shape[1], n_splits)) for model_name in models}\n",
    "\n",
    "# Perform cross-validation and collect predictions\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(features, targets)):\n",
    "    X_train, y_train = features.iloc[train_idx], targets.iloc[train_idx]\n",
    "    X_valid, y_valid = features.iloc[valid_idx], targets.iloc[valid_idx]\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training and predicting with model: {model_name}\")\n",
    "        clf = model\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_valid = clf.predict(X_valid)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        # Save OOF predictions\n",
    "        oof_preds[model_name][valid_idx] = y_pred_valid\n",
    "        # Save test predictions\n",
    "        test_preds[model_name][:,:,fold] = y_pred_test\n",
    "\n",
    "# Define the loss function for optimization\n",
    "def mse_loss(weights):\n",
    "    weights = np.array(weights)\n",
    "    # Normalize weights to sum to 1\n",
    "    weights = weights / np.sum(weights)\n",
    "    # Combine OOF predictions using the weights\n",
    "    final_oof = np.zeros_like(targets.values)\n",
    "    for i, model_name in enumerate(models):\n",
    "        final_oof += weights[i] * oof_preds[model_name]\n",
    "    # Compute mean squared error\n",
    "    mse = mean_squared_error(targets.values, final_oof)\n",
    "    return mse\n",
    "\n",
    "# Optimization methods to try\n",
    "#methods = [\n",
    "#    'Nelder-Mead', 'Powell', 'trust-constr', 'CG', 'BFGS', 'Newton-CG',\n",
    "#    'L-BFGS-B', 'TNC', 'COBYLA', 'SLSQP', 'dogleg', 'trust-ncg',\n",
    "#    'trust-exact', 'trust-krylov'\n",
    "#]\n",
    "methods = [\n",
    "    'Nelder-Mead', 'Powell',  'CG', 'BFGS',\n",
    "    'L-BFGS-B', 'TNC', 'SLSQP', \n",
    "] # 'trust-constr' is just really slow and not performing well at the this moment\n",
    "\n",
    "# Initial weights\n",
    "initial_weights = np.ones(len(models)) / len(models)\n",
    "\n",
    "# Constraints and bounds\n",
    "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "bounds = [(0, 1)] * len(models)\n",
    "\n",
    "# Optimize weights using different methods\n",
    "best_mse = np.inf\n",
    "best_weights = None\n",
    "best_method = None\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"Optimizing weights using method: {method}\")\n",
    "    try:\n",
    "        if method in ['trust-constr', 'COBYLA', 'SLSQP', 'trust-ncg', 'trust-krylov', 'trust-exact']:\n",
    "            res = minimize(mse_loss, initial_weights, method=method, bounds=bounds, constraints=constraints)\n",
    "        elif method in ['L-BFGS-B', 'TNC']:\n",
    "            res = minimize(mse_loss, initial_weights, method=method, bounds=bounds)\n",
    "        else:\n",
    "            # For unconstrained methods, weights will be normalized in mse_loss\n",
    "            res = minimize(mse_loss, initial_weights, method=method)\n",
    "        if res.fun < best_mse:\n",
    "            best_mse = res.fun\n",
    "            best_weights = res.x / np.sum(res.x)  # Normalize weights\n",
    "            best_method = method\n",
    "        print(f\"Method: {method}, MSE: {res.fun}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Method: {method}, failed with error: {e}\")\n",
    "\n",
    "# Average test predictions over folds for each model\n",
    "for model_name in models:\n",
    "    test_preds[model_name] = np.mean(test_preds[model_name], axis=2)  # Average over folds\n",
    "\n",
    "# Combine the test predictions using the best weights\n",
    "final_test_pred = np.zeros((X_test.shape[0], targets.shape[1]))\n",
    "for i, model_name in enumerate(models):\n",
    "    final_test_pred += best_weights[i] * test_preds[model_name]\n",
    "\n",
    "# Create a DataFrame with IDs and predictions\n",
    "prediction_df = pd.DataFrame({\n",
    "    'ID': test_final['ID'],\n",
    "    'longitude_predicted': final_test_pred[:, 1],\n",
    "    'latitude_predicted': final_test_pred[:, 0]\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "prediction_df.to_csv('predictions10.csv', index=False)\n",
    "\n",
    "# Print the best method and weights\n",
    "print(f\"Best optimization method: {best_method}\")\n",
    "print(\"Best weights:\")\n",
    "for i, model_name in enumerate(models):\n",
    "    print(f\"{model_name}: {best_weights[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4149950074734456\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
