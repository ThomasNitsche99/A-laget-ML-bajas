{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/ais_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/ais_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#train =train.drop(['cog','sog','rot','heading','navstat','portId','etaRaw'], axis=1)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m train \u001b[38;5;241m=\u001b[39mtrain\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportId\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124metaRaw\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlproject/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlproject/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlproject/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlproject/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlproject/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/ais_train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/raw/ais_train.csv', sep='|')\n",
    "#train =train.drop(['cog','sog','rot','heading','navstat','portId','etaRaw'], axis=1)\n",
    "train =train.drop(['portId','etaRaw'], axis=1)\n",
    "train['time'] = pd.to_datetime(train['time'])\n",
    "train = train.sort_values(by=['vesselId','time'])\n",
    "train.head()\n",
    "\n",
    "# Load vessels data\n",
    "vessels = pd.read_csv('../data/cleaned/cleaned_vessels.csv', sep=',')\n",
    "vessels.head()\n",
    "\n",
    "# Merge with training data\n",
    "train = train.merge(vessels, on='vesselId', how='left')\n",
    "train.head()\n",
    "print(train.columns)\n",
    "\n",
    "\n",
    "# Handle missing values in vessel data (if any)\n",
    "# For demonstration, fill missing numerical values with median and categorical with mode\n",
    "numerical_cols = ['CEU', 'DWT', 'GT', 'NT', 'breadth', 'depth', 'draft', 'enginePower', 'maxHeight', 'maxSpeed', 'maxWidth', 'rampCapacity', 'yearBuilt']\n",
    "categorical_cols = ['vesselType', 'homePort', 'fuel', 'freshWater']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    train[col] = train[col].fillna(train[col].median())\n",
    "\n",
    "for col in categorical_cols:\n",
    "    train[col] = train[col].fillna(train[col].mode()[0])\n",
    "\n",
    "# Encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Combine train and test vesselType categories to avoid unseen categories in test set\n",
    "vessel_type_le = LabelEncoder()\n",
    "train['vesselType_encoded'] = vessel_type_le.fit_transform(train['vesselType'])\n",
    "\n",
    "home_port_le = LabelEncoder()\n",
    "train['homePort_encoded'] = home_port_le.fit_transform(train['homePort'])\n",
    "\n",
    "# Calculate vessel age at the time of each observation\n",
    "train['vessel_age'] = train['time'].dt.year - train['yearBuilt']\n",
    "\n",
    "# Drop original categorical columns if not needed\n",
    "#train = train.drop(['vesselType', 'homePort', 'fuel', 'freshWater', 'yearBuilt'], axis=1)\n",
    "\n",
    "# Reorder columns if desired\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[\"vesselId\"].value_counts())\n",
    "\n",
    "# modify train so it only contains vesselId: 6323f2287abc89c0a9631e57 and 61e9f466b937134a3c4c0273\n",
    "\n",
    "#train = train[train['vesselId'].isin(['6323f2287abc89c0a9631e57', '61e9f466b937134a3c4c0273'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(df: pd.DataFrame, N_KEEP_PAST: int) -> pd.DataFrame:\n",
    "    data_rows = []  # List to collect all the data rows\n",
    "    vessel_list = df['vesselId'].unique()\n",
    "    for vessel in tqdm(vessel_list):\n",
    "        vessel_data = df[df['vesselId'] == vessel].sort_values(by='time').reset_index(drop=True)\n",
    "        num_rows = vessel_data.shape[0]\n",
    "        if num_rows <= N_KEEP_PAST:\n",
    "            continue  # Skip vessels with insufficient data\n",
    "        for i in range(N_KEEP_PAST, num_rows):\n",
    "            # Collect past N_KEEP_PAST locations and timestamps\n",
    "            past_data = vessel_data.loc[i - N_KEEP_PAST:i - 1].reset_index(drop=True)\n",
    "            current_data = vessel_data.loc[i]\n",
    "            # Prepare a dictionary to hold the features and target\n",
    "            data_row = {}\n",
    "            target_time = current_data['time']\n",
    "            for j in range(N_KEEP_PAST):\n",
    "                past_time = past_data.loc[j, 'time']\n",
    "                # Calculate the difference in minutes between past time and target time\n",
    "                time_diff = (target_time - past_time).total_seconds() / 60.0  # Difference in minutes\n",
    "                data_row[f'minutes_from_target_{j}'] = time_diff\n",
    "                data_row[f'lat_{j}'] = past_data.loc[j, 'latitude']\n",
    "                data_row[f'lon_{j}'] = past_data.loc[j, 'longitude']\n",
    "                #######\n",
    "                data_row[f'cog_{j}'] = past_data.loc[j, 'cog']\n",
    "                data_row[f'sog_{j}'] = past_data.loc[j, 'sog']\n",
    "                data_row[f'rot_{j}'] = past_data.loc[j, 'rot']\n",
    "                data_row[f'heading_{j}'] = past_data.loc[j, 'heading']\n",
    "                data_row[f'navstat_{j}'] = past_data.loc[j, 'navstat']\n",
    "                ######\n",
    "            for col in ['CEU', 'DWT', 'GT', 'NT', 'breadth', 'depth', 'draft',\n",
    "                        'enginePower', 'maxHeight', 'maxSpeed', 'maxWidth', 'rampCapacity',\n",
    "                        'vesselType_encoded', 'homePort_encoded', 'vessel_age']:\n",
    "                data_row[col] = current_data[col]\n",
    "            # Add current location as the target\n",
    "            data_row['target_lat'] = current_data['latitude']\n",
    "            data_row['target_lon'] = current_data['longitude']\n",
    "            data_row['vesselId'] = vessel  # Include vesselId if needed\n",
    "            data_row['target_time'] = target_time # Remove eventually\n",
    "            # Append the row to the list\n",
    "            data_rows.append(data_row)\n",
    "    # Create final DataFrame from the list of data rows\n",
    "    final_df = pd.DataFrame(data_rows)\n",
    "    return final_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_KEEP_PAST = 5\n",
    "final_train = create_training_data(train, N_KEEP_PAST)\n",
    "final_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_train.shape)\n",
    "display(final_train.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/ais_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m test \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mmerge(vessels, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvesselId\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../data/raw/ais_test.csv')\n",
    "test['time'] = pd.to_datetime(test['time'])\n",
    "test = test.merge(vessels, on='vesselId', how='left')\n",
    "# Encode categorical variables in test data\n",
    "test['vesselType_encoded'] = vessel_type_le.transform(test['vesselType'])\n",
    "test['homePort_encoded'] = home_port_le.transform(test['homePort'])\n",
    "\n",
    "\n",
    "for col in numerical_cols:\n",
    "    test[col] = test[col].fillna(train[col].median())  # Use median from train\n",
    "\n",
    "for col in categorical_cols:\n",
    "    test[col] = test[col].fillna(train[col].mode()[0]) \n",
    "    \n",
    "if 'yearBuilt' in test.columns:\n",
    "    test['vessel_age'] = test['time'].dt.year - test['yearBuilt']\n",
    "else:\n",
    "    print(\"'yearBuilt' is not present in test data after merging.\")\n",
    "    # Decide how to handle this case\n",
    "    # For example, you can set a default vessel age\n",
    "    test['vessel_age'] = test['time'].dt.year - train['yearBuilt'].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "# modify test so it only contains vesselId: 6323f2287abc89c0a9631e57 and 61e9f466b937134a3c4c0273\n",
    "#test = test[test['vesselId'].isin(['6323f2287abc89c0a9631e57', '61e9f466b937134a3c4c0273'])]\n",
    "print(test.shape)\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_features(train_df: pd.DataFrame, test_df: pd.DataFrame, N_KEEP_PAST: int) -> pd.DataFrame:\n",
    "    data_rows = []  # List to collect all the data rows\n",
    "    vessel_list = test_df['vesselId'].unique()\n",
    "    \n",
    "    for vessel in tqdm(vessel_list):\n",
    "        # Get the test data for this vessel\n",
    "        test_vessel_data = test_df[test_df['vesselId'] == vessel]\n",
    "        for idx, test_row in test_vessel_data.iterrows():\n",
    "            target_time = test_row['time']\n",
    "            ID = test_row['ID']\n",
    "            scaling_factor = test_row['scaling_factor']  # If needed later\n",
    "            \n",
    "            # Get past data from train for this vessel before the target time\n",
    "            vessel_train_data = train_df[(train_df['vesselId'] == vessel) & (train_df['time'] < target_time)]\n",
    "            vessel_train_data = vessel_train_data.sort_values(by='time').reset_index(drop=True)\n",
    "            num_past_points = vessel_train_data.shape[0]\n",
    "            \n",
    "            if num_past_points < N_KEEP_PAST:\n",
    "                # Not enough past data; decide how to handle (skip or pad with NaNs)\n",
    "                continue  # Or handle as per your requirement\n",
    "            \n",
    "            # Get the last N_KEEP_PAST records\n",
    "            past_data = vessel_train_data.iloc[-N_KEEP_PAST:].reset_index(drop=True)\n",
    "            \n",
    "            # Prepare a dictionary to hold the features\n",
    "            data_row = {}\n",
    "            for j in range(N_KEEP_PAST):\n",
    "                past_time = past_data.loc[j, 'time']\n",
    "                # Calculate the difference in minutes between past time and target time\n",
    "                time_diff = (target_time - past_time).total_seconds() / 60.0  # Difference in minutes\n",
    "                data_row[f'minutes_from_target_{j}'] = time_diff\n",
    "                data_row[f'lat_{j}'] = past_data.loc[j, 'latitude']\n",
    "                data_row[f'lon_{j}'] = past_data.loc[j, 'longitude']\n",
    "                ###\n",
    "                data_row[f'cog_{j}'] = past_data.loc[j, 'cog']\n",
    "                data_row[f'sog_{j}'] = past_data.loc[j, 'sog']\n",
    "                data_row[f'rot_{j}'] = past_data.loc[j, 'rot']\n",
    "                data_row[f'heading_{j}'] = past_data.loc[j, 'heading']\n",
    "                data_row[f'navstat_{j}'] = past_data.loc[j, 'navstat']\n",
    "                ###\n",
    "            for col in ['CEU', 'DWT', 'GT', 'NT', 'breadth', 'depth', 'draft',\n",
    "                        'enginePower', 'maxHeight', 'maxSpeed', 'maxWidth', 'rampCapacity',\n",
    "                        'vesselType_encoded', 'homePort_encoded', 'vessel_age']:\n",
    "                data_row[col] = test_row[col]\n",
    "            # Include vesselId and ID for result matching\n",
    "            data_row['vesselId'] = vessel\n",
    "            data_row['ID'] = ID\n",
    "            # Append the row to the list\n",
    "            data_rows.append(data_row)\n",
    "    \n",
    "    # Create test features DataFrame from the list of data rows\n",
    "    test_features = pd.DataFrame(data_rows)\n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = create_test_features(train, test, N_KEEP_PAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train.to_csv('final_train.csv', index=False)\n",
    "test_final.to_csv('test_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Load the data\n",
    "test_final = pd.read_csv('test_final.csv')\n",
    "final_train = pd.read_csv('final_train.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Define features and targets\n",
    "features = final_train.drop(columns=['target_lat', 'target_lon', 'vesselId', 'target_time'])\n",
    "targets = final_train[['target_lat', 'target_lon']]\n",
    "\n",
    "feature_columns = features.columns\n",
    "X_test = test_final[feature_columns]\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'RandomForest': MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42, max_depth=4, n_jobs=8)),\n",
    "    'XGBoost': MultiOutputRegressor(xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42)),\n",
    "    'ElasticNet': MultiOutputRegressor(ElasticNet(random_state=42)),\n",
    "    'LightGBM': MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))\n",
    "}\n",
    "\n",
    "# Prepare arrays to hold OOF predictions and test predictions\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = {model_name: np.zeros((features.shape[0], targets.shape[1])) for model_name in models}\n",
    "test_preds = {model_name: np.zeros((X_test.shape[0], targets.shape[1], n_splits)) for model_name in models}\n",
    "\n",
    "# Perform cross-validation and collect predictions\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(features, targets)):\n",
    "    X_train, y_train = features.iloc[train_idx], targets.iloc[train_idx]\n",
    "    X_valid, y_valid = features.iloc[valid_idx], targets.iloc[valid_idx]\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training and predicting with model: {model_name}\")\n",
    "        clf = model\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_valid = clf.predict(X_valid)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        # Save OOF predictions\n",
    "        oof_preds[model_name][valid_idx] = y_pred_valid\n",
    "        # Save test predictions\n",
    "        test_preds[model_name][:,:,fold] = y_pred_test\n",
    "\n",
    "# Define the loss function for optimization\n",
    "def mse_loss(weights):\n",
    "    weights = np.array(weights)\n",
    "    # Normalize weights to sum to 1\n",
    "    weights = weights / np.sum(weights)\n",
    "    # Combine OOF predictions using the weights\n",
    "    final_oof = np.zeros_like(targets.values)\n",
    "    for i, model_name in enumerate(models):\n",
    "        final_oof += weights[i] * oof_preds[model_name]\n",
    "    # Compute mean squared error\n",
    "    mse = mean_squared_error(targets.values, final_oof)\n",
    "    return mse\n",
    "\n",
    "# Optimization methods to try\n",
    "#methods = [\n",
    "#    'Nelder-Mead', 'Powell', 'trust-constr', 'CG', 'BFGS', 'Newton-CG',\n",
    "#    'L-BFGS-B', 'TNC', 'COBYLA', 'SLSQP', 'dogleg', 'trust-ncg',\n",
    "#    'trust-exact', 'trust-krylov'\n",
    "#]\n",
    "methods = [\n",
    "    'Nelder-Mead', 'Powell',  'CG', 'BFGS',\n",
    "    'L-BFGS-B', 'TNC', 'SLSQP', \n",
    "] # 'trust-constr' is just really slow and not performing well at the this moment\n",
    "\n",
    "# Initial weights\n",
    "initial_weights = np.ones(len(models)) / len(models)\n",
    "\n",
    "# Constraints and bounds\n",
    "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "bounds = [(0, 1)] * len(models)\n",
    "\n",
    "# Optimize weights using different methods\n",
    "best_mse = np.inf\n",
    "best_weights = None\n",
    "best_method = None\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"Optimizing weights using method: {method}\")\n",
    "    try:\n",
    "        if method in ['trust-constr', 'COBYLA', 'SLSQP', 'trust-ncg', 'trust-krylov', 'trust-exact']:\n",
    "            res = minimize(mse_loss, initial_weights, method=method, bounds=bounds, constraints=constraints)\n",
    "        elif method in ['L-BFGS-B', 'TNC']:\n",
    "            res = minimize(mse_loss, initial_weights, method=method, bounds=bounds)\n",
    "        else:\n",
    "            # For unconstrained methods, weights will be normalized in mse_loss\n",
    "            res = minimize(mse_loss, initial_weights, method=method)\n",
    "        if res.fun < best_mse:\n",
    "            best_mse = res.fun\n",
    "            best_weights = res.x / np.sum(res.x)  # Normalize weights\n",
    "            best_method = method\n",
    "        print(f\"Method: {method}, MSE: {res.fun}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Method: {method}, failed with error: {e}\")\n",
    "\n",
    "# Average test predictions over folds for each model\n",
    "for model_name in models:\n",
    "    test_preds[model_name] = np.mean(test_preds[model_name], axis=2)  # Average over folds\n",
    "\n",
    "# Combine the test predictions using the best weights\n",
    "final_test_pred = np.zeros((X_test.shape[0], targets.shape[1]))\n",
    "for i, model_name in enumerate(models):\n",
    "    final_test_pred += best_weights[i] * test_preds[model_name]\n",
    "\n",
    "# Create a DataFrame with IDs and predictions\n",
    "prediction_df = pd.DataFrame({\n",
    "    'ID': test_final['ID'],\n",
    "    'longitude_predicted': final_test_pred[:, 1],\n",
    "    'latitude_predicted': final_test_pred[:, 0]\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "prediction_df.to_csv('predictions_ensemble2.csv', index=False)\n",
    "\n",
    "# Print the best method and weights\n",
    "print(f\"Best optimization method: {best_method}\")\n",
    "print(\"Best weights:\")\n",
    "for i, model_name in enumerate(models):\n",
    "    print(f\"{model_name}: {best_weights[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4149950074734456\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
